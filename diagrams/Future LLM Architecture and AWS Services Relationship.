// Future LLM Architecture and AWS Services Relationship
digraph {
	rankdir=LR
	moe [label="Mixture-of-Experts (MoE)"]
	mla [label="Multi-Head Latent Attention"]
	gqa [label="Grouped-Query Attention"]
	sliding [label="Sliding Window Attention"]
	ple [label="Per-Layer Embedding"]
	matformer [label="MatFormer/Matryoshka"]
	aws_moe [label="AWS MoE Accelerator"]
	aws_param [label="AWS Parameter Streaming"]
	aws_expert [label="AWS Expert Orchestration"]
	aws_attention [label="AWS Attention Optimizer"]
	aws_inferentia [label="AWS Inferentia Next-Gen"]
	aws_memory [label="AWS Memory Hierarchy"]
	aws_param_store [label="AWS Tiered Parameter Store"]
	aws_neuron [label="AWS Neuron SDK Enhancements"]
	aws_slicing [label="AWS Model Slicing Service"]
	moe -> aws_moe [label=enables]
	moe -> aws_param [label=requires]
	moe -> aws_expert [label="managed by"]
	mla -> aws_attention [label="optimized by"]
	gqa -> aws_attention [label="optimized by"]
	sliding -> aws_attention [label="optimized by"]
	mla -> aws_inferentia [label="accelerated by"]
	gqa -> aws_inferentia [label="accelerated by"]
	sliding -> aws_inferentia [label="accelerated by"]
	ple -> aws_memory [label="managed by"]
	matformer -> aws_slicing [label="enabled by"]
	ple -> aws_param_store [label=utilizes]
	matformer -> aws_neuron [label="optimized by"]
}
